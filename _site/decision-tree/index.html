<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Decision Tree - Blog</title>
<meta name="description" content="Machine Learning">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Blog">
<meta property="og:title" content="Decision Tree">
<meta property="og:url" content="http://localhost:4000/CodingSpace/decision-tree/">


  <meta property="og:description" content="Machine Learning">



  <meta property="og:image" content="http://localhost:4000/CodingSpace/assets/images/algorithm/cnn.jpeg">



  <meta name="twitter:site" content="@Manikan09676833">
  <meta name="twitter:title" content="Decision Tree">
  <meta name="twitter:description" content="Machine Learning">
  <meta name="twitter:url" content="http://localhost:4000/CodingSpace/decision-tree/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="http://localhost:4000/CodingSpace/assets/images/algorithm/cnn.jpeg">
    
  

  



  <meta property="article:published_time" content="2020-01-22T00:00:00+05:30">





  

  


<link rel="canonical" href="http://localhost:4000/CodingSpace/decision-tree/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Manikant Kumar",
      "url": "http://localhost:4000/CodingSpace",
      "sameAs": ["https://facebook.com","https://twitter.com/Manikan09676833"]
    }
  </script>







<!-- end _includes/seo.html -->


<link href="http://localhost:4000/CodingSpace/feed.xml" type="application/atom+xml" rel="alternate" title="Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/CodingSpace/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/CodingSpace/">Blog</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/CodingSpace/about/" >About</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      
  











<div class="page__hero--overlay"
  style="background-color: #333; "
>
  
    <div class="wrapper">
      <h1 class="page__title" itemprop="headline">
        
          Decision Tree

        
      </h1>
      
        <p class="page__lead">Machine Learning
</p>
      
      
      
    </div>
  
  
</div>




  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="http://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
          <a href="http://localhost:4000/CodingSpace/" itemprop="item"><span itemprop="name">Home</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        <li class="current">Decision Tree</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Decision Tree">
    <meta itemprop="description" content="Machine Learning">
    <meta itemprop="datePublished" content="January 22, 2020">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#decision-tree">Decision Tree</a>
    <ul>
      <li><a href="#classification-trees">Classification Trees</a></li>
      <li><a href="#maths-behind-decision-tree-classifier">Maths behind Decision Tree Classifier</a></li>
      <li><a href="#different-algorithms-for-decision-tree">Different Algorithms for Decision Tree</a></li>
      <li><a href="#bias-variance-tradeoff-for-k-fold-cv-loocv-and-holdout-set-cv">Bias Variance tradeoff for k-fold CV, LOOCV and Holdout Set CV</a></li>
      <li><a href="#implementation-in-python">Implementation in Python</a></li>
      <li><a href="#what-are-hyper-parameters">What are hyper parameters?</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <h2 id="decision-tree">Decision Tree</h2>

<p>Decision tree algorithm is one of the most versatile algorithms in machine learning which can perform both classification and regression analysis. It is very powerful and works great with complex datasets. Apart from that, it is very easy to understand and read. That makes it more popular to use. When coupled with ensemble techniques – which we will learn very soon- it performs even better.
As the name suggests, this algorithm works by dividing the whole dataset into a tree-like structure based on some rules and conditions and then gives prediction based on those conditions.
Let’s understand the approach to decision tree with a basic scenario. 
Suppose it’s Friday night and you are not able to decide if you should go out or stay at home. Let the decision tree decide it for you.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/Decision_tree1.png" width="300" /></p>

<p>Although we may or may not use the decision tree for such decisions, this was a basic example to help you understand how a decision tree makes a decision.
So how did it work?</p>
<ul>
  <li>It selects a root node based on a given condition, e.g. our root node was chosen as time &gt;10 pm.</li>
  <li>Then, the root node was split into child notes based on the given condition. The right child node in the above figure fulfilled the condition, so no more questions were asked.</li>
  <li>The left child node didn’t fulfil the condition, so again it was split based on a new condition.</li>
  <li>This process continues till all the conditions are met or if you have predefined the depth of your tree, e.g. the depth of our tree is 3, and it reached there when all the conditions were exhausted.</li>
</ul>

<p>Let’s see how the parent nodes and condition is chosen for the splitting to work.</p>

<h4 id="decision-tree-for-regression">Decision Tree for Regression</h4>
<p>When performing regression with a decision tree, we try to divide the given values of X into distinct and non-overlapping regions, e.g. for a set of possible values X1, X2,…, Xp; we will try to divide them into J distinct and non-overlapping regions R1, R2, . . . , RJ.
For a given observation falling into the region Rj, the prediction is equal to the mean of the response(y) values for each training observations(x) in the region Rj. 
The regions R1,R2, . . . , RJ  are selected in a way to reduce the following sum of squares of residuals:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/formula1.png" width="300" /></p>

<p>Where, yrj (second term) is the mean of all the response variables in the region ‘j’.</p>

<h4 id="recursive-binary-splittinggreedy-approach">Recursive binary splitting(Greedy approach)</h4>
<p>As mentioned above, we try to divide the X values into j regions, but it is very expensive in terms of computational time to try to fit every set of X values into j regions. Thus, decision tree opts for a top-down greedy approach in which nodes are divided into two regions based on the given condition, i.e. not every node will be split but the ones which satisfy the condition are split into two branches. It is called greedy because it does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps. It decides a threshold value(say s) to divide the observations into different regions(j) such that the RSS for Xj&gt;= s and Xj &lt;s is minimum.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/formula2.png" width="400" /></p>

<p>Here for the above equation, j and s are found such that this equation has the minimum value.
The regions R1, R2 are selected based on that value of s and j such that the equation above has the minimum value.
Similarly, more regions are split out of the regions created above based on some condition with the same logic. This continues until a stopping criterion (predefined) is achieved.
Once all the regions are split, the prediction is made based on the mean of observations in that region.</p>

<p>The process mentioned above has a high chance of overfitting the training data as it will be very complex.</p>

<h4 id="tree-pruning">Tree Pruning</h4>
<p>Tree pruning is the method of trimming down a full tree (obtained through the above process) to reduce the complexity and variance in the data. Just as we regularized linear regression, we can also regularize the decision tree model by adding a new term.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/formula3.png" width="300" /></p>

<p>Where, T  is the subtree which is a subset of the full tree T0
And α is the non-negative tuning parameter which penalises the MSE with an increase in tree length.
By using cross-validation, such values of α and T are selected for which our model gives the lowest test error rate.
This is how the decision tree regression model works. Let’s now see the working algorithm of doing classification using a decision tree.
Greedy Algorithm
As per Hands-on machine learning book “greedy algorithm greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.”</p>

<h4 id="post-pruning">Post-pruning</h4>

<p>Post-pruning, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed. Cross-validation set of data is used to check the effect of pruning and tests whether expanding a node will make an improvement or not. If any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node.</p>

<h4 id="pre-pruning">Pre-pruning</h4>

<p>Pre-pruning, also known as forward pruning, stops the non-significant branches from generating. It uses a condition to decide when should it terminate splitting of some of the branches prematurely as the tree is generated.</p>

<h3 id="classification-trees">Classification Trees</h3>

<p>Regression trees are used for quantitative data. In the case of qualitative data or categorical data, we use classification trees.  In regression trees, we split the nodes based on RSS criteria, but in classification, it is done using classification error rate, Gini impurity and entropy.
Let’s understand these terms in detail.</p>

<h4 id="entropy">Entropy</h4>
<p>Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/entropy.png" width="300" /></p>

<p>When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop. 
A region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present (high entropy).
Let’s suppose there are ‘m’ observations and we need to classify them into categories 1 and 2.
Let’s say that category 1 has ‘n’ observations and category 2 has ‘m-n’ observations.</p>

<p>p= n/m  and    q = m-n/m = 1-p</p>

<p>then, entropy for the given set is:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      E = -p*log2(p) – q*log2(q) 
</code></pre></div></div>

<p>When all the observations belong to category 1, then p = 1 and all observations belong to category 2, then p =0, int both cases E =0, as there is no randomness in the categories.
If half of the observations are in category 1 and another half in category 2, then p =1/2 and q =1/2, and the entropy is maximum, E =1.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/entropy1.png" width="300" /></p>

<h4 id="information-gain">Information Gain</h4>
<p>Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/info_gain.png" width="300" /></p>

<p>Where, T is the parent node before split and X is the split node from T.</p>

<p>A tree which is splitted on basis of entropy and information gain value looks like:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/entropy_tree.png" width="900" /></p>

<h4 id="ginni-impurity">Ginni Impurity</h4>
<p>According to wikipedia, ‘Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’
It is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class.
Let’s suppose there are k number of classes and an observation belongs to the class ‘i’, then Ginni impurity is given as:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/ginni.png" width="300" /></p>

<p>Ginni impurity value lies between 0 and 1, 0 being no impurity and 1 denoting random distribution.
The node for which the Ginni impurity is least is selected as the root node to split.</p>

<p>A tree which is splitted on basis of ginni impurity value looks like:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/tree_example.PNG" width="900" /></p>

<h3 id="maths-behind-decision-tree-classifier">Maths behind Decision Tree Classifier</h3>
<p>Before we see the python implementation of decision tree. let’s first understand the math behind the decision tree classfication. We will see how all the above mentioned terms are used for splitting.</p>

<p>We will use a simple dataset which contains information about students of different classes and gender and see whether they stay in school’s hostel or not.</p>

<p>This is how our data set looks like :</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/data_class.PNG" width="200" /></p>

<p>Let’s try and understand how the root node is selected by calcualting gini impurity. We will use the above mentioned data.</p>

<p>We have two features which we can use for nodes: “Class” and “Gender”.
We will calculate gini impurity for each of the features and then select that feature which has least gini impurity.</p>

<p>Let’s review the formula for calculating ginni impurity:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/gini.PNG" width="200" /></p>

<p>Let’s start with class, we will try to gini impurity for all different values in “class”.</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/1.png" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/2.png" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/3.1.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/3.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/4.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/5.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/6.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/7.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/8.PNG" width="500" /></p>

<p>This is how our Decision tree node is selected by calculating gini impurity for each node individually.
If the number of features increases, then we just need to repeat the same steps after the selection of the root node.</p>

<p>We will try and find the root nodes for the same dataset by calculating entropy and information gain.</p>

<p>DataSet:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/data_class.PNG" width="200" /></p>

<p>We have two features and we will try to choose the root node by calculating the information gain by splitting each feature.</p>

<p>Let’ review the formula for entropy and information gain:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/formula_entropy.PNG" width="300" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/inform_gain.PNG" width="300" /></p>

<p>Let’s start with feature “class” :</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/9.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/10.1.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/11.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/12.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/13.PNG" width="500" /></p>

<p>Let’ see the information gain from feature “gender” :</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/10.2.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/14.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/15.PNG" width="500" /></p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/16.PNG" width="500" /></p>

<h3 id="different-algorithms-for-decision-tree">Different Algorithms for Decision Tree</h3>

<ul>
  <li>
    <p>ID3 (Iterative Dichotomiser) : It is one of the algorithms used to construct decision tree for classification. It uses Information gain as the criteria for finding the root nodes and splitting them. It only accepts categorical attributes.</p>
  </li>
  <li>
    <p>C4.5 : It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values.It is also used for classfication purposes.</p>
  </li>
  <li>
    <p>Classfication and Regression Algorithm(CART) : It is the most popular algorithm used for constructing decison trees. It uses ginni impurity as the default calculation for selecting root nodes, however one can use “entropy” for criteria as well. This algorithm works on both regression as well as classfication problems. We will use this algorithm in our pyhton implementation.</p>
  </li>
</ul>

<p>Entropy and Ginni impurity can be used reversibly. It doesn’t affects the result much. Although, ginni is easier to compute than entropy, since entropy has a log term calculation. That’s why CART algorithm uses ginni as the default algorithm.</p>

<p>If we plot ginni vs entropy graph, we can see there is not much difference between them:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/example/entropyVsgini.PNG" width="400" /></p>

<h5 id="advantages-of-decision-tree">Advantages of Decision Tree:</h5>

<ul>
  <li>It can be used for both Regression and Classification problems.</li>
  <li>Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.</li>
  <li>Complex decision tree models are very simple when visualized. It can be understood just by visualising.</li>
  <li>Scaling and normalization are not needed.</li>
</ul>

<h5 id="disadvantages-of-decision-tree">Disadvantages of Decision Tree:</h5>

<ul>
  <li>A small change in data can cause instability in the model because of the greedy approach.</li>
  <li>Probability of overfitting is very high for Decision Trees.</li>
  <li>It takes more time to train a decision tree model than other classification algorithms.</li>
</ul>

<h1 id="cross-validation">Cross-Validation</h1>

<p>Suppose you train a model on a given dataset using any specific algorithm. You tried to find the accuracy of the trained model using the same training data and found the accuracy to be 95% or maybe even 100%. What does this mean? Is your model ready for prediction? The answer is no.
Why? Because your model has trained itself on the given data, i.e. it knows the data and it has generalized over it very well. But when you try and predict over a new set of data, it’s most likely to give you very bad accuracy, because it has never seen the data before and thus it fails to generalizes well over it. This is the problem of overfitting. 
To tackle such problem, Cross-validation comes into the picture. Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it. If the model works with good accuracy on your test data, it means that the model has not overfitted the training data and can be trusted with the prediction, whereas if it performs with bad accuracy then our model is not to be trusted and we need to tweak our algorithm.</p>

<p>Let’s see the different approaches of Cross-Validation:</p>

<ul>
  <li>Hold Out Method:</li>
</ul>

<p>It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.</p>

<ul>
  <li>k-fold Cross-Validation</li>
</ul>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/cv1.png" width="" /></p>

<p>img_src:Wikipedia</p>

<p>To tackle the high variance of Hold-out method, the k-fold method is used. The idea is simple, divide the whole dataset into ‘k’ sets preferably of equal sizes. Then the first set is selected as the test set and the rest ‘k-1’ sets are used to train the data. Error is calculated for this particular dataset.
Then the steps are repeated, i.e. the second set is selected as the test data, and the remaining ‘k-1’ sets are used as the training data. Again, the error is calculated. Similarly, the process continues for ‘k’ times. In the end, the CV error is given as the mean of the total errors calculated individually, mathematically given as:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/cv2.png" width="" /></p>

<p>The variance in error decreases with the increase in ‘k’. The disadvantage of k-fold cv is that it is computationally expensive as the algorithm runs from scratch for ‘k’ times.</p>

<ul>
  <li>Leave One Out Cross Validation (LOOCV)</li>
</ul>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/cv3.png" width="" /></p>

<p>LOOCV is a special case of k-fold CV, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation. This process continues ‘n’ times and in the end, CV error is calculated as:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/cv4.png" width="" /></p>

<h3 id="bias-variance-tradeoff-for-k-fold-cv-loocv-and-holdout-set-cv">Bias Variance tradeoff for k-fold CV, LOOCV and Holdout Set CV</h3>

<p>There is a very good explanation given in the ISLR Book as given below:</p>

<p>A k-fold CV with k &lt; n has a computational advantage to LOOCV. But putting computational issues aside,
a less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV.
The validation set approach can lead to overestimates of the test error rate since in this approach the
the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. And performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias since each training set contains (k − 1)n/k observations—fewer than
in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k &lt; n. Why
is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k-fold CV with k &lt; n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.</p>

<h3 id="implementation-in-python">Implementation in Python</h3>

<p>we will use Sklearn module to implement decision tree algorithm. 
Sklearn uses CART (classification and Regression trees) algorithm and by default it uses Gini impurity as a criteria to split the nodes.</p>

<p>There are other algorithms like ID3, C4.5, Chi-square etc.</p>

<p>We will see the use of CART in following implementation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">graphviz</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">export_graphviz</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span><span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.externals.six</span> <span class="kn">import</span> <span class="n">StringIO</span>  
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>  
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>
<span class="kn">import</span> <span class="nn">pydotplus</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"winequality_red.csv"</span><span class="p">)</span>
<span class="n">data</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>7.4</td>
      <td>0.700</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.99780</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>1</td>
      <td>7.8</td>
      <td>0.880</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.99680</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <td>2</td>
      <td>7.8</td>
      <td>0.760</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.99700</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <td>3</td>
      <td>11.2</td>
      <td>0.280</td>
      <td>0.56</td>
      <td>1.9</td>
      <td>0.075</td>
      <td>17.0</td>
      <td>60.0</td>
      <td>0.99800</td>
      <td>3.16</td>
      <td>0.58</td>
      <td>9.8</td>
      <td>6</td>
    </tr>
    <tr>
      <td>4</td>
      <td>7.4</td>
      <td>0.700</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.99780</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>1594</td>
      <td>6.2</td>
      <td>0.600</td>
      <td>0.08</td>
      <td>2.0</td>
      <td>0.090</td>
      <td>32.0</td>
      <td>44.0</td>
      <td>0.99490</td>
      <td>3.45</td>
      <td>0.58</td>
      <td>10.5</td>
      <td>5</td>
    </tr>
    <tr>
      <td>1595</td>
      <td>5.9</td>
      <td>0.550</td>
      <td>0.10</td>
      <td>2.2</td>
      <td>0.062</td>
      <td>39.0</td>
      <td>51.0</td>
      <td>0.99512</td>
      <td>3.52</td>
      <td>0.76</td>
      <td>11.2</td>
      <td>6</td>
    </tr>
    <tr>
      <td>1596</td>
      <td>6.3</td>
      <td>0.510</td>
      <td>0.13</td>
      <td>2.3</td>
      <td>0.076</td>
      <td>29.0</td>
      <td>40.0</td>
      <td>0.99574</td>
      <td>3.42</td>
      <td>0.75</td>
      <td>11.0</td>
      <td>6</td>
    </tr>
    <tr>
      <td>1597</td>
      <td>5.9</td>
      <td>0.645</td>
      <td>0.12</td>
      <td>2.0</td>
      <td>0.075</td>
      <td>32.0</td>
      <td>44.0</td>
      <td>0.99547</td>
      <td>3.57</td>
      <td>0.71</td>
      <td>10.2</td>
      <td>5</td>
    </tr>
    <tr>
      <td>1598</td>
      <td>6.0</td>
      <td>0.310</td>
      <td>0.47</td>
      <td>3.6</td>
      <td>0.067</td>
      <td>18.0</td>
      <td>42.0</td>
      <td>0.99549</td>
      <td>3.39</td>
      <td>0.66</td>
      <td>11.0</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
<p>1599 rows × 12 columns</p>
</div>

<p>The data set consists following Input variables :
1 - fixed acidity  2 - volatile acidity  3 - citric acid  4 - residual sugar  5 - chlorides  6 - free sulfur dioxide</p>

<p>7 - total sulfur dioxide  8 - density  9 - pH   10 - sulphates   11 - alcohol</p>

<p>and the Output variable gives the quality of th wine based on the input variables:</p>

<p>12 - quality (score between 0 and 10)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
      <td>1599.000000</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>8.319637</td>
      <td>0.527821</td>
      <td>0.270976</td>
      <td>2.538806</td>
      <td>0.087467</td>
      <td>15.874922</td>
      <td>46.467792</td>
      <td>0.996747</td>
      <td>3.311113</td>
      <td>0.658149</td>
      <td>10.422983</td>
      <td>5.636023</td>
    </tr>
    <tr>
      <td>std</td>
      <td>1.741096</td>
      <td>0.179060</td>
      <td>0.194801</td>
      <td>1.409928</td>
      <td>0.047065</td>
      <td>10.460157</td>
      <td>32.895324</td>
      <td>0.001887</td>
      <td>0.154386</td>
      <td>0.169507</td>
      <td>1.065668</td>
      <td>0.807569</td>
    </tr>
    <tr>
      <td>min</td>
      <td>4.600000</td>
      <td>0.120000</td>
      <td>0.000000</td>
      <td>0.900000</td>
      <td>0.012000</td>
      <td>1.000000</td>
      <td>6.000000</td>
      <td>0.990070</td>
      <td>2.740000</td>
      <td>0.330000</td>
      <td>8.400000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>7.100000</td>
      <td>0.390000</td>
      <td>0.090000</td>
      <td>1.900000</td>
      <td>0.070000</td>
      <td>7.000000</td>
      <td>22.000000</td>
      <td>0.995600</td>
      <td>3.210000</td>
      <td>0.550000</td>
      <td>9.500000</td>
      <td>5.000000</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>7.900000</td>
      <td>0.520000</td>
      <td>0.260000</td>
      <td>2.200000</td>
      <td>0.079000</td>
      <td>14.000000</td>
      <td>38.000000</td>
      <td>0.996750</td>
      <td>3.310000</td>
      <td>0.620000</td>
      <td>10.200000</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>9.200000</td>
      <td>0.640000</td>
      <td>0.420000</td>
      <td>2.600000</td>
      <td>0.090000</td>
      <td>21.000000</td>
      <td>62.000000</td>
      <td>0.997835</td>
      <td>3.400000</td>
      <td>0.730000</td>
      <td>11.100000</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <td>max</td>
      <td>15.900000</td>
      <td>1.580000</td>
      <td>1.000000</td>
      <td>15.500000</td>
      <td>0.611000</td>
      <td>72.000000</td>
      <td>289.000000</td>
      <td>1.003690</td>
      <td>4.010000</td>
      <td>2.000000</td>
      <td>14.900000</td>
      <td>8.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>We can see there is no missing data in the columns. Great!!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="s">'quality'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'quality'</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span><span class="p">,</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">355</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#let's first visualize the tree on the data without doing any pre processing</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=None, splitter='best')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_name</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">class_name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">feature_name</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['fixed acidity',
 'volatile acidity',
 'citric acid',
 'residual sugar',
 'chlorides',
 'free sulfur dioxide',
 'total sulfur dioxide',
 'density',
 'pH',
 'sulphates',
 'alcohol']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create a dot_file which stores the tree structure</span>
<span class="n">dot_data</span> <span class="o">=</span> <span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">feature_name</span><span class="p">,</span><span class="n">rounded</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span><span class="n">filled</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c"># Draw graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">pydotplus</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>  
<span class="n">graph</span><span class="o">.</span><span class="n">write_png</span><span class="p">(</span><span class="s">"myTree.png"</span><span class="p">)</span>
<span class="c"># Show graph</span>
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">create_png</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/output_20_0.png" alt="png" /></p>

<p>Let’s understand the above tree:</p>

<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/tree_explain.PNG" width="300" /></p>

<ul>
  <li>the first value indicates the column and the condition on which the root node was selected and further will be splitted</li>
  <li>the second value gives the gini impurity of the selected node</li>
  <li>samples gives the number of observations at that point of time present in the node</li>
  <li>value within the square brackets represents number of observations present in each class(output) i.e. in the above given figure, 8 observations are in class 1, 38 in class 2 , 468 in class 3 and so on.</li>
</ul>

<p>Then  the split was made on the basis of given condition.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.0
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">py_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># accuracy of our classification tree</span>
<span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.5791666666666667
</code></pre></div></div>

<p>Now we haven’t done any prerocessing with our data and neither done any hyper parameter tunings.Let’s do all those and see how our score improves.</p>

<h3 id="what-are-hyper-parameters">What are hyper parameters?</h3>
<p><img src="https://devil-cyber.github.io/CodingSpace/assets/images/hypr_params.PNG" width="700" /></p>

<p>We can see above the decision tree classifier algorithm takes all those parameters which are also known as hyperparameters.</p>

<p>Let’s see the most important ones of the parameters(as per sklearn documentation) :</p>
<h4 id="parameters">Parameters</h4>
<hr />
<ul>
  <li>
    <p>criterion : string, optional (default=”gini”)
    The function to measure the quality of a split. Supported criteria are
    “gini” for the Gini impurity and “entropy” for the information gain.</p>
  </li>
  <li>
    <p>splitter : string, optional (default=”best”)
   The strategy used to choose the split at each node. Supported
   strategies are “best” to choose the best split and “random” to choose
   the best random split.</p>
  </li>
  <li>
    <p>max_depth : int or None, optional (default=None)
   The maximum depth of the tree. If None, then nodes are expanded until
   all leaves are pure or until all leaves contain less than
   min_samples_split samples.</p>
  </li>
  <li>
    <p>min_samples_split : int, float, optional (default=2)
   The minimum number of samples required to split an internal node:</p>

    <ul>
      <li>If int, then consider <code class="highlighter-rouge">min_samples_split</code> as the minimum number.</li>
      <li>If float, then <code class="highlighter-rouge">min_samples_split</code> is a fraction and
<code class="highlighter-rouge">ceil(min_samples_split * n_samples)</code> are the minimum
number of samples for each split.</li>
    </ul>

    <p>.. versionchanged:: 0.18
      Added float values for fractions.</p>
  </li>
  <li>
    <p>min_samples_leaf : int, float, optional (default=1)
   The minimum number of samples required to be at a leaf node.
   A split point at any depth will only be considered if it leaves at
   least <code class="highlighter-rouge">min_samples_leaf</code> training samples in each of the left and
   right branches.  This may have the effect of smoothing the model,
   especially in regression.</p>

    <ul>
      <li>If int, then consider <code class="highlighter-rouge">min_samples_leaf</code> as the minimum number.</li>
      <li>If float, then <code class="highlighter-rouge">min_samples_leaf</code> is a fraction and
<code class="highlighter-rouge">ceil(min_samples_leaf * n_samples)</code> are the minimum
number of samples for each node.</li>
    </ul>
  </li>
  <li>
    <p>max_features : int, float, string or None, optional (default=None)
   The number of features to consider when looking for the best split:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   - If int, then consider `max_features` features at each split.
   - If float, then `max_features` is a fraction and
     `int(max_features * n_features)` features are considered at each
     split.
   - If "auto", then `max_features=sqrt(n_features)`.
   - If "sqrt", then `max_features=sqrt(n_features)`.
   - If "log2", then `max_features=log2(n_features)`.
   - If None, then `max_features=n_features`.
</code></pre></div>    </div>

    <p>Note: the search for a split does not stop until at least one
   valid partition of the node samples is found, even if it requires to
   effectively inspect more than <code class="highlighter-rouge">max_features</code> features.</p>
  </li>
  <li>
    <p>random_state : int, RandomState instance or None, optional (default=None)
   If int, random_state is the seed used by the random number generator;
   If RandomState instance, random_state is the random number generator;
   If None, the random number generator is the RandomState instance used
   by <code class="highlighter-rouge">np.random</code>.</p>
  </li>
  <li>
    <p>max_leaf_nodes : int or None, optional (default=None)
   Grow a tree with <code class="highlighter-rouge">max_leaf_nodes</code> in best-first fashion.
   Best nodes are defined as relative reduction in impurity.
   If None then unlimited number of leaf nodes.</p>
  </li>
  <li>
    <p>min_impurity_decrease : float, optional (default=0.)
   A node will be split if this split induces a decrease of the impurity
   greater than or equal to this value.</p>
  </li>
  <li>
    <p>min_impurity_split : float, (default=1e-7)
   Threshold for early stopping in tree growth. A node will split
   if its impurity is above the threshold, otherwise it is a leaf.</p>
  </li>
  <li>
    <p>class_weight : dict, list of dicts, “balanced” or None, default=None
   Weights associated with classes in the form <code class="highlighter-rouge">{class_label: weight}</code>.
   If not given, all classes are supposed to have weight one. For
   multi-output problems, a list of dicts can be provided in the same
   order as the columns of y.</p>
  </li>
  <li>
    <p>presort : bool, optional (default=False)
    Whether to presort the data to speed up the finding of best splits in
    fitting. For the default settings of a decision tree on large
    datasets, setting this to true may slow down the training process.
    When using either a smaller dataset or a restricted depth, this may
    speed up the training.</p>
  </li>
</ul>

<p>When we do hyperparameter tuning, we basically try to find those sets and values of hyperparameters which will give us a model with maximum accuracy.
Let’s go ahead and try to improve our model.</p>

<p>We will start with scaling our data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scalar</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">x_transform</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span><span class="p">,</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x_transform</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">355</span><span class="p">)</span>
</code></pre></div></div>

<p>Although our dataset is realtively small, let’s use PCA for feature selection and see if it improves our accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">principalComponents</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_transform</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of Components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Variance (</span><span class="si">%</span><span class="s">)'</span><span class="p">)</span> <span class="c">#for each component</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Explained Variance'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="output_30_0.png" alt="png" /></p>

<p>We can see that around 95% of the variance is being explained by 8 components.
So instead of giving all 11 columns as input in our algorithm let’s use these 8 principal components instead.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">new_data</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_transform</span><span class="p">)</span>

<span class="n">principal_x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'PC-1'</span><span class="p">,</span><span class="s">'PC-2'</span><span class="p">,</span><span class="s">'PC-3'</span><span class="p">,</span><span class="s">'PC-4'</span><span class="p">,</span><span class="s">'PC-5'</span><span class="p">,</span><span class="s">'PC-6'</span><span class="p">,</span><span class="s">'PC-7'</span><span class="p">,</span><span class="s">'PC-8'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">principal_x</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC-1</th>
      <th>PC-2</th>
      <th>PC-3</th>
      <th>PC-4</th>
      <th>PC-5</th>
      <th>PC-6</th>
      <th>PC-7</th>
      <th>PC-8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>-1.619530</td>
      <td>0.450950</td>
      <td>-1.774454</td>
      <td>0.043740</td>
      <td>0.067014</td>
      <td>-0.913921</td>
      <td>-0.161043</td>
      <td>-0.282258</td>
    </tr>
    <tr>
      <td>1</td>
      <td>-0.799170</td>
      <td>1.856553</td>
      <td>-0.911690</td>
      <td>0.548066</td>
      <td>-0.018392</td>
      <td>0.929714</td>
      <td>-1.009829</td>
      <td>0.762587</td>
    </tr>
    <tr>
      <td>2</td>
      <td>-0.748479</td>
      <td>0.882039</td>
      <td>-1.171394</td>
      <td>0.411021</td>
      <td>-0.043531</td>
      <td>0.401473</td>
      <td>-0.539553</td>
      <td>0.597946</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2.357673</td>
      <td>-0.269976</td>
      <td>0.243489</td>
      <td>-0.928450</td>
      <td>-1.499149</td>
      <td>-0.131017</td>
      <td>0.344290</td>
      <td>-0.455375</td>
    </tr>
    <tr>
      <td>4</td>
      <td>-1.619530</td>
      <td>0.450950</td>
      <td>-1.774454</td>
      <td>0.043740</td>
      <td>0.067014</td>
      <td>-0.913921</td>
      <td>-0.161043</td>
      <td>-0.282258</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>1594</td>
      <td>-2.150500</td>
      <td>0.814286</td>
      <td>0.617063</td>
      <td>0.407687</td>
      <td>-0.240936</td>
      <td>0.054835</td>
      <td>0.170812</td>
      <td>-0.355866</td>
    </tr>
    <tr>
      <td>1595</td>
      <td>-2.214496</td>
      <td>0.893101</td>
      <td>1.807402</td>
      <td>0.414003</td>
      <td>0.119592</td>
      <td>-0.674711</td>
      <td>-0.607970</td>
      <td>-0.247640</td>
    </tr>
    <tr>
      <td>1596</td>
      <td>-1.456129</td>
      <td>0.311746</td>
      <td>1.124239</td>
      <td>0.491877</td>
      <td>0.193716</td>
      <td>-0.506410</td>
      <td>-0.231082</td>
      <td>0.079382</td>
    </tr>
    <tr>
      <td>1597</td>
      <td>-2.270518</td>
      <td>0.979791</td>
      <td>0.627965</td>
      <td>0.639770</td>
      <td>0.067735</td>
      <td>-0.860408</td>
      <td>-0.321487</td>
      <td>-0.468876</td>
    </tr>
    <tr>
      <td>1598</td>
      <td>-0.426975</td>
      <td>-0.536690</td>
      <td>1.628955</td>
      <td>-0.391716</td>
      <td>0.450482</td>
      <td>-0.496154</td>
      <td>1.189132</td>
      <td>0.042176</td>
    </tr>
  </tbody>
</table>
<p>1599 rows × 8 columns</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let's see how well our model perform on this new data</span>
<span class="n">x_train</span><span class="p">,</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">principal_x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">355</span><span class="p">)</span>
<span class="c">#let's first visualize the tree on the data without doing any pre processing</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.58125
</code></pre></div></div>

<p>There is a little increase in our test accuracy. Great!!</p>

<p>Let’s now try to tune some hyperparameters using the GridSearchCV algorithm.
We have studied about CrossValidation in upcoming lecture.</p>

<p>GridSearchCV is a method used to tune our hyperparameters. We can pass different values of hyperparameters as parameters for grid search.
It does a exhaustive generation of combination of different parameters passed.
Using cross validation score, Grid Search returns the combination of hyperparameters for which the model is performing the best.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># we are tuning three hyperparameters right now, we are passing the different values for both parameters</span>
<span class="n">grid_param</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'criterion'</span><span class="p">:</span> <span class="p">[</span><span class="s">'gini'</span><span class="p">,</span> <span class="s">'entropy'</span><span class="p">],</span>
    <span class="s">'max_depth'</span> <span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
    <span class="s">'min_samples_leaf'</span> <span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
    <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
    <span class="s">'splitter'</span> <span class="p">:</span> <span class="p">[</span><span class="s">'best'</span><span class="p">,</span> <span class="s">'random'</span><span class="p">]</span>
    
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span>
                     <span class="n">param_grid</span><span class="o">=</span><span class="n">grid_param</span><span class="p">,</span>
                     <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                    <span class="n">n_jobs</span> <span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>C:\Users\user\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
  DeprecationWarning)





GridSearchCV(cv=5, error_score='raise-deprecating',
             estimator=DecisionTreeClassifier(class_weight=None,
                                              criterion='entropy', max_depth=16,
                                              max_features=None,
                                              max_leaf_nodes=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              presort=False, random_state=None,
                                              splitter='best'),
             iid='warn', n_jobs=-1,
             param_grid={'criterion': ['gini', 'entropy'],
                         'max_depth': range(2, 32),
                         'min_samples_leaf': range(1, 10),
                         'min_samples_split': range(2, 10),
                         'splitter': ['best', 'random']},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=None, verbose=0)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_parameters</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="k">print</span><span class="p">(</span><span class="n">best_parameters</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'criterion': 'entropy', 'max_depth': 24, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.5933869526362824
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span> <span class="o">=</span> <span class="s">'entropy'</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">splitter</span> <span class="o">=</span><span class="s">'random'</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=24,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=None, splitter='random')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.6041666666666666
</code></pre></div></div>

<p>Great!! Our test score has improved after using Gridsearch.</p>

<p>Note : we must understand that giving all the hyperparameters in the gridSearch doesn’t gurantee the best result. We have to do hit and trial with parameters to get the perfect score.</p>

<p>You are welcome to try tweaking the parameters more and try an improve the accuracy more.</p>

<p>Let’s visualize the tree:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_name</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">class_name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="c"># create a dot_file which stores the tree structure</span>
<span class="n">dot_data</span> <span class="o">=</span> <span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">rounded</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span><span class="n">filled</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c"># Draw graph</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">pydotplus</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>  
<span class="c">#graph.write_png("tree.png")</span>
<span class="c"># Show graph</span>
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">create_png</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="output_46_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let's save the model</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'C://Users//user/devil/Decision tree'</span><span class="o">+</span> <span class="s">'/modelForPrediction.sav'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>
    
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'C://Users//user/devil/Decision tree'</span><span class="o">+</span> <span class="s">'/standardScalar.sav'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'C://Users//user/Devil/Decision tree'</span><span class="o">+</span> <span class="s">'/pca_model.sav'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div>


        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2020-01-22T00:00:00+05:30">January 22, 2020</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=Manikan09676833&text=Decision+Tree%20http%3A%2F%2Flocalhost%3A4000%2FCodingSpace%2Fdecision-tree%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FCodingSpace%2Fdecision-tree%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  
</section>


      
  <nav class="pagination">
    
      <a href="http://localhost:4000/CodingSpace/programming/" class="pagination--pager" title="Basic Mathematical Algorithm Used in Programming
">Previous</a>
    
    
      <a href="http://localhost:4000/CodingSpace/cnn/" class="pagination--pager" title="Deep Dive To Convolution Neural Network
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "https://devil-cyber.github.io/CodingSpace/assets/images/algorithm/tf.png"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/CodingSpace/TensorFlow/" rel="permalink">Get Started With Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Deep Learning &amp; Machine Learning
</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/CodingSpace/assets/images/algorithm/page.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/CodingSpace/pagerank/" rel="permalink">Understanding PageRank
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Indexing the web pages
</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/CodingSpace/assets/images/algorithm/mnist.png"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/CodingSpace/mnist/" rel="permalink">Hand Written Digit Classification Using PyTorch
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Deep Learning
</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "http://localhost:4000/CodingSpace/assets/images/algorithm/cf.jpeg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/CodingSpace/confusion/" rel="permalink">Confusion Matrix is not so confusing 😂
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Machine Learning
</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap">
  <input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  <div id="results" class="results"></div>
</div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/Manikan09676833"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="https://github.com/devil-cyber"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="http://localhost:4000/CodingSpace/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Manikant Kumar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="http://localhost:4000/CodingSpace/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>



  
  
  <script src="http://localhost:4000/CodingSpace/assets/js/lunr/lunr.min.js"></script>
  <script src="http://localhost:4000/CodingSpace/assets/js/lunr/lunr-store.js"></script>
  <script src="http://localhost:4000/CodingSpace/assets/js/lunr/lunr-en.js"></script>





    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/CodingSpace/decision-tree/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/DecisionTree"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://devil-cyber.github.io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>
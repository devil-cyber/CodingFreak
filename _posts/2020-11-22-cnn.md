---
layout: single
header:
  overlay_color: "#333"
  teaser: /assets/images/algorithm/cnn.jpeg
title:  "Deep Dive To Convolution Neural Network"
excerpt: "Deep Learning"
breadcrumbs: true
share: true
permalink: /cnn/
date:    2020-11-22
toc: false

---

LeNet is a `convolutional neural network structure` proposed by [Yann Lecun](http://yann.lecun.com/) in 1998. It was the first big breakthrough in the world of `deep learning and computer vision`. In general, LeNet refers to LeNet-5.This architecture was the first time applied over the [MNIST](http://yann.lecun.com/exdb/mnist/)  dataset to recognize the handwritten character. 

![gif](https://cdn.inblog.in/user/uploads/RxR3t6nBckHfcw9hcc8kn8Ejv3zOIw.gif)
<h1 style="color:grey">Architecture</h1>
![](https://cdn.inblog.in/user/uploads/76b146e6845d5f3736612ddc87abcfb9.png)

**Input Layer** -The size of the input image is uniformly normalized to 32 * 32.

**Convolutions** - Convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. This is related to a form of mathematical convolution. The matrix operation being performed—convolution—is not traditional matrix multiplication, despite being similarly denoted by *  
![](https://cdn.inblog.in/user/uploads/69da556f0c6149519ae07b7ad2fdb84b.png)

**Subsampling** -  It means **pooling** ,its function is to progressively reduce the spatial size of the representation to reduce the number of parameters and computation in the network.  

![](https://cdn.inblog.in/user/uploads/gqQkaDYMf937O4fZKgvwfNYzgFznAA.gif)

<h1 style="color:grey">Architecture Consist Of Following Layers</h1>
- `C1 layer-convolutional layer`
- `S2 layer-pooling layer` 
- `C3 layer-convolutional layer`
- `S4 layer-pooling layer`
- `C5 layer-convolution layer`
- `F6 layer-fully connected layer or output layer` - The output layer is also a fully connected layer, with a total of 10 nodes, which respectively represent the numbers 0 to 9, and if the value of node i is 0, the result of network recognition is the number
<h1 style="color:grey">Sample Code</h1>
```python

import tensorflow as tf
from tensorflow import keras
from keras.layers import Conv2D,MaxPool2D,Flatten,Dense
from keras.datasets import mnist

# Data preprocessing
(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train=x_train.reshape(x_train.shape[0],28,28,1)
x_test=x_test.reshape(x_test.shape[0],28,28,1)
x_train=x_train/225
x_test=x_test/225
y_train=keras.utils.to_categorical(y_train,10)
y_test=keras.utils.to_categorical(y_test,10)
 
# model 
model = keras.Sequential()

model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))
model.add(layers.AveragePooling2D())

model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
model.add(layers.AveragePooling2D())

model.add(layers.Flatten())

model.add(layers.Dense(units=120, activation='relu'))

model.add(layers.Dense(units=84, activation='relu'))

model.add(layers.Dense(units=10, activation = 'softmax'))
# model compile
model.compile(optimizer="adam",loss='categorical_crossentropy',metrics=['accuracy'])
# model fitting
model.fit(x_train,y_train,batch_size=128,epochs=10,validation_data=(x_test,y_test),verbose=1)

score=classifier.evaluate(x_test,y_test)
print('Test loss:',score[0])
print('Test acuuracy:',score[1])
# prediction
pred=classifier.predict(x_test[index].reshape(1,28,28,1))
print(pred.argmax())


```